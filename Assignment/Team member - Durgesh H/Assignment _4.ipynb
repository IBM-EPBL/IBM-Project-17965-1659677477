
Problem Statement: Customer Segmentation Analysis
Problem Statement You own the mall and want to understand the customers who can quickly converge [Target Customers] so that the insight can be given to the marketing team and plan the strategy accordingly. Perform the given Tasks to complete the assignment:- Clustering the data and performing classification algorithms

1. Download the dataset: Dataset
The Dataset for the assignment 4 is been downloaded and saved in .csv format

the dataset used here is Mall_Customers.csv

Importing required packages
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

import warnings
warnings.filterwarnings('ignore')
2. Load the dataset into the tool.
#import the dataset (Mall_Customers dataset)

df = pd.read_csv('C:\\Mall_Customers.csv')
df
CustomerID	Gender	Age	Annual Income (k$)	Spending Score (1-100)
0	1	Male	19	15	39
1	2	Male	21	15	81
2	3	Female	20	16	6
3	4	Female	23	16	77
4	5	Female	31	17	40
...	...	...	...	...	...
195	196	Female	35	120	79
196	197	Female	45	126	28
197	198	Male	32	126	74
198	199	Male	32	137	18
199	200	Male	30	137	83
200 rows × 5 columns

df.info()
RangeIndex: 200 entries, 0 to 199
Data columns (total 5 columns):
 #   Column                  Non-Null Count  Dtype 
---  ------                  --------------  ----- 
 0   CustomerID              200 non-null    int64 
 1   Gender                  200 non-null    object
 2   Age                     200 non-null    int64 
 3   Annual Income (k$)      200 non-null    int64 
 4   Spending Score (1-100)  200 non-null    int64 
dtypes: int64(4), object(1)
memory usage: 7.9+ KB
numerical_features = df.select_dtypes(include = [np.number]).columns
categorical_features = df.select_dtypes(include = [np.object]).columns
numerical_features
Index(['CustomerID', 'Age', 'Annual Income (k$)', 'Spending Score (1-100)'], dtype='object')
categorical_features
Index(['Gender'], dtype='object')
 
3. Perform Below Visualizations.
df.corr()
#plotting the correlation
plt.figure(1)
sns.heatmap(df.corr(), annot = True)

data=df
data = np.random.randint(low=1,high=100,size=(10,10))
print(data)
[[10 40 65 71  4 84 80 22 66 28]
 [45 73 95 56 28 67 60 78 96  9]
 [74 75 45 93 14 62 68 33 60 66]
 [43 65 83 62 25 37 56 59  2 16]
 [30  4 94 31  2 54  6 89 87 14]
 [26 99 60 88 40 59 98 54 51 72]
 [40 60 40 29 28 49  8  8 40 89]
 [36 82  8 73 52 71 19 32 62 42]
 [62  2 87  2 74  4 21 38 79 60]
 [95 12 31 15 29 41 75 56 83 43]]
 
 
∙ Univariate Analysis
#we have one categorical variable. ('Gender')
#plotting to check how many categories for 'Gender' variable
sns.countplot(df.Gender , data = df, palette = 'Set2')

This graph shows most of the customers are female

sns.distplot(df['Annual Income (k$)'])

sns.distplot(df['Spending Score (1-100)'])

sns.distplot(df['Age'])

 
∙ Bi- Variate Analysis
#exploring the Gender & Spending Score
plt.figure(3, figsize=(12, 10))
g = sns.FacetGrid(data = df, col= 'Gender', height = 4)
g.map(sns.distplot, 'Spending Score (1-100)')

#exploring the Gender & Annual Income
plt.figure(3, figsize=(12, 10))
g = sns.FacetGrid(data = df, col= 'Gender', height = 4)
g.map(sns.distplot, 'Annual Income (k$)')

#Plotting the features of the dataset to see the correlation between Number of Customer and Ages
age18_25 = df.Age[(df.Age <= 25) & (df.Age >= 18)]
age26_35 = df.Age[(df.Age <= 35) & (df.Age >= 26)]
age36_45 = df.Age[(df.Age <= 45) & (df.Age >= 36)]
age46_55 = df.Age[(df.Age <= 55) & (df.Age >= 46)]
age55above = df.Age[df.Age >= 56]
x = ["18-25","26-35","36-45","46-55","55+"]
y = [len(age18_25.values),len(age26_35.values),len(age36_45.values),len(age46_55.values),len(age55above.values)]
plt.figure(figsize=(6,6))
sns.barplot(x=x, y=y, palette="rocket")
plt.title("Number of Customer and Ages")
plt.xlabel("Age")
plt.ylabel("Number of Customer")
plt.show()

#Plotting the features of the dataset to see the correlation between Spending Scores ie Score & Number of Customer 
ss1_20 = df["Spending Score (1-100)"][(df["Spending Score (1-100)"] >= 1) & (df["Spending Score (1-100)"] <= 20)]
ss21_40 = df["Spending Score (1-100)"][(df["Spending Score (1-100)"] >= 21) & (df["Spending Score (1-100)"] <= 40)]
ss41_60 = df["Spending Score (1-100)"][(df["Spending Score (1-100)"] >= 41) & (df["Spending Score (1-100)"] <= 60)]
ss61_80 = df["Spending Score (1-100)"][(df["Spending Score (1-100)"] >= 61) & (df["Spending Score (1-100)"] <= 80)]
ss81_100 = df["Spending Score (1-100)"][(df["Spending Score (1-100)"] >= 81) & (df["Spending Score (1-100)"] <= 100)]
ssx = ["1-20", "21-40", "41-60", "61-80", "81-100"]
ssy = [len(ss1_20.values), len(ss21_40.values), len(ss41_60.values), len(ss61_80.values), len(ss81_100.values)]
plt.figure(figsize=(6,6))
sns.barplot(x=ssx, y=ssy, palette="rocket")
plt.title("Spending Scores")
plt.xlabel("Score")
plt.ylabel("Number of Customer Having the Score")
plt.show()

 
∙ Multi-Variate Analysis
# Plotting the features of the dataset to see the correlation between them

plt.hist(x = df.Gender, bins = 3, color = 'navy')
plt.title('comparison of male and female')
plt.xlabel('Spending Score (1-100)')
plt.ylabel('Annual Income (k$)')
plt.show()

plt.figure(figsize=(10,10))
sns.barplot(df['Age'], df['Spending Score (1-100)'], hue = df['Gender'], palette = 'pastel')
plt.title('Age vs Spending Score and Gender', fontsize = 20)
Text(0.5, 1.0, 'Age vs Spending Score and Gender')

 
3-D Plotting
plt.figure(figsize=(20,20))

import plotly.express as px

fig = px.scatter_3d(df, x='Age', y='Gender', z='Annual Income (k$)',#hue='Gender')

color='Spending Score (1-100)')

fig.show()
newplot.png

 
●Pairplot Analysis
sns.pairplot(df,hue='Gender',size=2)

sns.pairplot(df,hue='Age',size=2)

 
● Histogram visualisation
# Histogram visualisation for each attribute to know what kind of distribution it is?
df.hist(figsize=(20,10), grid=False, layout=(2, 2), bins = 30 , color = "pink" )
array([[,
        ],
       [,
        ]],
      dtype=object)

 
4. Perform descriptive statistics on the dataset.
df.describe()
CustomerID	Age	Annual Income (k$)	Spending Score (1-100)
count	200.000000	200.000000	200.000000	200.000000
mean	100.500000	38.850000	60.560000	50.200000
std	57.879185	13.969007	26.264721	25.823522
min	1.000000	18.000000	15.000000	1.000000
25%	50.750000	28.750000	41.500000	34.750000
50%	100.500000	36.000000	61.500000	50.000000
75%	150.250000	49.000000	78.000000	73.000000
max	200.000000	70.000000	137.000000	99.000000
df.describe().T
count	mean	std	min	25%	50%	75%	max
CustomerID	200.0	100.50	57.879185	1.0	50.75	100.5	150.25	200.0
Age	200.0	38.85	13.969007	18.0	28.75	36.0	49.00	70.0
Annual Income (k$)	200.0	60.56	26.264721	15.0	41.50	61.5	78.00	137.0
Spending Score (1-100)	200.0	50.20	25.823522	1.0	34.75	50.0	73.00	99.0
df.columns
Index(['CustomerID', 'Gender', 'Age', 'Annual Income (k$)',
       'Spending Score (1-100)'],
      dtype='object')
df.dtypes
CustomerID                 int64
Gender                    object
Age                        int64
Annual Income (k$)         int64
Spending Score (1-100)     int64
dtype: object
df.var()
CustomerID                3350.000000
Age                        195.133166
Annual Income (k$)         689.835578
Spending Score (1-100)     666.854271
dtype: float64
df.skew()
CustomerID                0.000000
Age                       0.485569
Annual Income (k$)        0.321843
Spending Score (1-100)   -0.047220
dtype: float64
df.corr()
CustomerID	Age	Annual Income (k$)	Spending Score (1-100)
CustomerID	1.000000	-0.026763	0.977548	0.013835
Age	-0.026763	1.000000	-0.012398	-0.327227
Annual Income (k$)	0.977548	-0.012398	1.000000	0.009903
Spending Score (1-100)	0.013835	-0.327227	0.009903	1.000000
df.std()
CustomerID                57.879185
Age                       13.969007
Annual Income (k$)        26.264721
Spending Score (1-100)    25.823522
dtype: float64
 
5. Check for Missing values and deal with them.
#checking for missing values
df.isna().sum() 
df.info()
RangeIndex: 200 entries, 0 to 199
Data columns (total 5 columns):
 #   Column                  Non-Null Count  Dtype 
---  ------                  --------------  ----- 
 0   CustomerID              200 non-null    int64 
 1   Gender                  200 non-null    object
 2   Age                     200 non-null    int64 
 3   Annual Income (k$)      200 non-null    int64 
 4   Spending Score (1-100)  200 non-null    int64 
dtypes: int64(4), object(1)
memory usage: 7.9+ KB
#checking the duplicate values
df.duplicated().sum()
0
#checking the null values
df.isnull().sum()
CustomerID                0
Gender                    0
Age                       0
Annual Income (k$)        0
Spending Score (1-100)    0
dtype: int64
sns.heatmap(df.isnull(),yticklabels=False,cbar=False)

This Dataset contain no null values
 
6. Find the outliers and replace them outliers
Visualization
Boxplot
boxplot is a visualization tool for identifying outliers

fig,ax=plt.subplots(figsize=(25,5))

plt.subplot(1, 5, 2)
sns.boxplot(x=df['Age'])

plt.subplot(1, 5, 3)
sns.boxplot(x=df['Annual Income (k$)'])

plt.subplot(1, 5, 4)
sns.boxplot(x=df['Spending Score (1-100)'])

plt.subplot(1, 5, 1)
sns.boxplot(x=df['CustomerID'])

Histogram
fig,ax=plt.subplots(figsize=(25,5))

plt.subplot(1, 5, 1)
plt.hist(x=df['Age'],color = "red" )

plt.subplot(1, 5, 2)
plt.hist(x=df['Annual Income (k$)'],color = "navy" )

plt.subplot(1, 5, 3)
plt.hist(x=df['Spending Score (1-100)'],color = "darkgreen" )
(array([16., 20., 10., 17., 35., 37., 11., 24., 14., 16.]),
 array([ 1. , 10.8, 20.6, 30.4, 40.2, 50. , 59.8, 69.6, 79.4, 89.2, 99. ]),
 )

the histogram appears to be distributed to the left, this also indicates the presence of outliers.

Skewness
print('skewness value of Age: ',df['Age'].skew())
skewness value of Age:  0.48556885096681657
print('skewness value of Annual Income: ',df['Annual Income (k$)'].skew())
skewness value of Annual Income:  0.3218425498619055
print('skewness value of Spending Score: ',df['Spending Score (1-100)'].skew())
skewness value of Spending Score:  -0.047220201374263374
the skewness value should be within the range of -1 to 1 for a normal distribution, any major changes from this value indicates the presence of extreme value or outlier.

Interquartile Range
quantile = df.quantile(q = [0.25, 0.75])
quantile
CustomerID	Age	Annual Income (k$)	Spending Score (1-100)
0.25	50.75	28.75	41.5	34.75
0.75	150.25	49.00	78.0	73.00
quantile.loc[0.75]
CustomerID                150.25
Age                        49.00
Annual Income (k$)         78.00
Spending Score (1-100)     73.00
Name: 0.75, dtype: float64
quantile.loc[0.25]
CustomerID                50.75
Age                       28.75
Annual Income (k$)        41.50
Spending Score (1-100)    34.75
Name: 0.25, dtype: float64
IQR = quantile.iloc[1] - quantile.iloc[0]
IQR
CustomerID                99.50
Age                       20.25
Annual Income (k$)        36.50
Spending Score (1-100)    38.25
dtype: float64
upper = quantile.iloc[1] + (1.5 *IQR)
upper
CustomerID                299.500
Age                        79.375
Annual Income (k$)        132.750
Spending Score (1-100)    130.375
dtype: float64
lower = quantile.iloc[0] - (1.5* IQR)
lower
CustomerID               -98.500
Age                       -1.625
Annual Income (k$)       -13.250
Spending Score (1-100)   -22.625
dtype: float64
Standard Deviation
df.mean()
CustomerID                100.50
Age                        38.85
Annual Income (k$)         60.56
Spending Score (1-100)     50.20
dtype: float64
Outliers Treatment
df['Annual Income (k$)'].max()
137
sns.boxplot(df['CustomerID'])

sns.boxplot(df['Age'])

sns.boxplot(df['Annual Income (k$)'])

df['Annual Income (k$)'] = np.where(df['Annual Income (k$)'] > 132.750, 60.55, df['Annual Income (k$)'])
sns.boxplot(df['Annual Income (k$)'])

df['Annual Income (k$)'].max()
126.0
sns.boxplot(df['Spending Score (1-100)'])

Now most of the outliers be treated

 
7. Check for Categorical columns and perform encoding.
Encoding Categorical variables into numerical variables

df.select_dtypes(include='object').columns
Index(['Gender'], dtype='object')
df['Gender'].unique()
array(['Male', 'Female'], dtype=object)
df['Gender'].replace({'Male':1,'Female':0},inplace=True)
df.head()
CustomerID	Gender	Age	Annual Income (k$)	Spending Score (1-100)
0	1	1	19	15.0	39
1	2	1	21	15.0	81
2	3	0	20	16.0	6
3	4	0	23	16.0	77
4	5	0	31	17.0	40
 
8. Scaling the data
from sklearn.preprocessing import MinMaxScaler
scale = MinMaxScaler()
x_scaled = pd.DataFrame(scale.fit_transform(df),columns= df.columns)
x_scaled.head()
CustomerID	Gender	Age	Annual Income (k$)	Spending Score (1-100)
0	0.000000	1.0	0.019231	0.000000	0.387755
1	0.005025	1.0	0.057692	0.000000	0.816327
2	0.010050	0.0	0.038462	0.009009	0.051020
3	0.015075	0.0	0.096154	0.009009	0.775510
4	0.020101	0.0	0.250000	0.018018	0.397959
 
 
9. Perform any of the clustering algorithms
from sklearn.cluster import KMeans
error = []
k = list(range(2,9))

for i in k:
  kmeans = KMeans(n_clusters = i , init = 'k-means++')
  kmeans.fit(df)
  error.append(kmeans.inertia_)
error
[381507.64738523844,
 268062.5543374743,
 191550.08627670945,
 153530.68956249495,
 119166.15727643932,
 101320.93600180371,
 85744.90139221888]
plt.plot(k,error, 'ro--')
plt.xlabel('No of Clusters')
plt.ylabel('error')
Text(0, 0.5, 'error')

model = KMeans(n_clusters = 4)
model.fit(df)
KMeans(n_clusters=4)
mb=pd.Series(model.labels_)
df.head(3)
CustomerID	Gender	Age	Annual Income (k$)	Spending Score (1-100)
0	1	1	19	15.0	39
1	2	1	21	15.0	81
2	3	0	20	16.0	6
 
10. Add the cluster data with the primary dataset
df['Cluster']=kmeans.labels_
df.head()
CustomerID	Gender	Age	Annual Income (k$)	Spending Score (1-100)	Cluster
0	1	1	19	15.0	39	5
1	2	1	21	15.0	81	0
2	3	0	20	16.0	6	5
3	4	0	23	16.0	77	0
4	5	0	31	17.0	40	5
df.tail()
CustomerID	Gender	Age	Annual Income (k$)	Spending Score (1-100)	Cluster
195	196	0	35	120.00	79	3
196	197	0	45	126.00	28	7
197	198	1	32	126.00	74	3
198	199	1	32	60.55	18	7
199	200	1	30	60.55	83	3
 
11. Split the data into dependent and independent variables.
X=df.drop('Cluster',axis=1)
y=df['Cluster']
y
0      5
1      0
2      5
3      0
4      5
      ..
195    3
196    7
197    3
198    7
199    3
Name: Cluster, Length: 200, dtype: int32
 
12. Split the data into training and testing
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)
print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)
(160, 5)
(160,)
(40, 5)
(40,)
X_train
CustomerID	Gender	Age	Annual Income (k$)	Spending Score (1-100)
79	80	0	49	54.0	42
197	198	1	32	126.0	74
38	39	0	36	37.0	26
24	25	0	54	28.0	14
122	123	0	40	69.0	58
...	...	...	...	...	...
106	107	0	66	63.0	50
14	15	1	37	20.0	13
92	93	1	48	60.0	49
179	180	1	35	93.0	90
102	103	1	67	62.0	59
160 rows × 5 columns

X_test
CustomerID	Gender	Age	Annual Income (k$)	Spending Score (1-100)
95	96	1	24	60.0	52
15	16	1	22	20.0	79
30	31	1	60	30.0	4
158	159	1	34	78.0	1
128	129	1	59	71.0	11
115	116	0	19	65.0	50
69	70	0	32	48.0	47
170	171	1	40	87.0	13
174	175	0	52	88.0	13
45	46	0	24	39.0	65
66	67	0	43	48.0	50
182	183	1	46	98.0	15
165	166	0	36	85.0	75
78	79	0	23	54.0	52
186	187	0	54	101.0	24
177	178	1	27	88.0	69
56	57	0	51	44.0	50
152	153	0	44	78.0	20
82	83	1	67	54.0	41
68	69	1	19	48.0	59
124	125	0	23	70.0	29
16	17	0	35	21.0	35
148	149	0	34	78.0	22
93	94	0	40	60.0	40
65	66	1	18	48.0	59
60	61	1	70	46.0	56
84	85	0	21	54.0	57
67	68	0	68	48.0	48
125	126	0	31	70.0	77
132	133	0	25	72.0	34
9	10	0	30	19.0	72
18	19	1	52	23.0	29
55	56	1	47	43.0	41
75	76	1	26	54.0	54
150	151	1	43	78.0	17
104	105	1	49	62.0	56
135	136	0	29	73.0	88
137	138	1	32	73.0	73
164	165	1	50	85.0	26
76	77	0	45	54.0	53
y_train
79     2
197    3
38     5
24     5
122    4
      ..
106    4
14     5
92     4
179    3
102    4
Name: Cluster, Length: 160, dtype: int32
y_test
95     4
15     0
30     5
158    1
128    1
115    4
69     2
170    7
174    7
45     0
66     2
182    7
165    3
78     4
186    7
177    3
56     2
152    1
82     2
68     2
124    1
16     5
148    1
93     4
65     2
60     2
84     4
67     2
125    6
132    1
9      0
18     5
55     2
75     2
150    1
104    4
135    6
137    6
164    7
76     2
Name: Cluster, dtype: int32
 
13. Build the Model
# model building
from sklearn.naive_bayes import GaussianNB
model =GaussianNB()
model.fit(X_train,y_train)
GaussianNB()
 
14. Train the Model
y_pred = model.predict(X_train)
y_pred
array([2, 3, 5, 5, 4, 3, 0, 0, 6, 4, 4, 3, 0, 1, 4, 2, 1, 4, 7, 4, 3, 4,
       0, 5, 0, 4, 2, 3, 4, 1, 2, 6, 1, 0, 4, 4, 5, 6, 4, 4, 5, 2, 2, 3,
       3, 4, 6, 5, 7, 2, 5, 6, 4, 0, 0, 7, 6, 5, 0, 4, 5, 5, 1, 6, 4, 1,
       5, 2, 4, 1, 7, 5, 7, 6, 2, 6, 2, 2, 6, 5, 2, 4, 6, 1, 0, 0, 7, 6,
       4, 0, 7, 2, 4, 1, 2, 1, 0, 4, 2, 6, 7, 7, 3, 2, 2, 5, 0, 4, 4, 4,
       7, 3, 4, 5, 0, 2, 3, 6, 0, 2, 3, 1, 3, 3, 2, 2, 4, 2, 7, 2, 2, 4,
       0, 2, 1, 7, 6, 0, 6, 3, 0, 2, 6, 1, 6, 4, 4, 4, 4, 2, 4, 3, 5, 7,
       2, 4, 5, 4, 3, 4])
model.score(X_train,y_train)
1.0
 
15. Test the Model
y_pred = model.predict(X_test)
y_pred
array([4, 0, 5, 1, 1, 4, 2, 7, 7, 0, 2, 7, 3, 4, 7, 3, 2, 1, 2, 2, 1, 5,
       1, 4, 2, 2, 4, 2, 6, 1, 0, 5, 2, 2, 1, 4, 6, 6, 7, 2])
model.score(X_test,y_test)
1.0
 
16. Measure the performance using Evaluation Metrics.
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
accuracy_score(y_test,y_pred)
1.0
y_pred1 = model.predict(X_train)

accuracy_score(y_train,y_pred1)
1.0
y_pred=model.predict(X_test)
confusion_matrix(y_test,y_pred)
array([[ 3,  0,  0,  0,  0,  0,  0,  0],
       [ 0,  7,  0,  0,  0,  0,  0,  0],
       [ 0,  0, 11,  0,  0,  0,  0,  0],
       [ 0,  0,  0,  2,  0,  0,  0,  0],
       [ 0,  0,  0,  0,  6,  0,  0,  0],
       [ 0,  0,  0,  0,  0,  3,  0,  0],
       [ 0,  0,  0,  0,  0,  0,  3,  0],
       [ 0,  0,  0,  0,  0,  0,  0,  5]], dtype=int64)
pd.crosstab(y_test,y_pred)
col_0	0	1	2	3	4	5	6	7
Cluster								
0	3	0	0	0	0	0	0	0
1	0	7	0	0	0	0	0	0
2	0	0	11	0	0	0	0	0
3	0	0	0	2	0	0	0	0
4	0	0	0	0	6	0	0	0
5	0	0	0	0	0	3	0	0
6	0	0	0	0	0	0	3	0
7	0	0	0	0	0	0	0	5
print(classification_report(y_test,y_pred))
              precision    recall  f1-score   support

           0       1.00      1.00      1.00         3
           1       1.00      1.00      1.00         7
           2       1.00      1.00      1.00        11
           3       1.00      1.00      1.00         2
           4       1.00      1.00      1.00         6
           5       1.00      1.00      1.00         3
           6       1.00      1.00      1.00         3
           7       1.00      1.00      1.00         5

    accuracy                           1.00        40
   macro avg       1.00      1.00      1.00        40
weighted avg       1.00      1.00      1.00        40

 
